{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "source = r\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\DEEP-CHEF-PROJECT\\\\arym\\\\csv\\\\links.csv\"\n",
    "target = r\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\Desktop\\\\img2rec\\\\DEEP-CHEF-PROJECT\\\\aarush\\\\linksc.csv\"\n",
    "\n",
    "shutil.copyfile(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "import urllib3\n",
    "#335 to 668 incl\n",
    "#train folder and name of rceipe folder to be swapped in path\n",
    "\n",
    "def fetch_image_content_from_url(url,timeout):\n",
    "    try:\n",
    "        http = urllib3.PoolManager(timeout=urllib3.Timeout(connect=None, read=None, total=timeout))\n",
    "        response = http.request('GET', url)\n",
    "        return response.data\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred inside fetch_image_content_from_url\\n\",e)\n",
    "        \n",
    "        \n",
    "def download_image(download_path,url,filename):\n",
    "    try:\n",
    "        image_content = fetch_image_content_from_url(url,5)\n",
    "        #requests.get(url,timeout=10)\n",
    "        \"\"\"if(img_content.status_code==200):\n",
    "            image_content=img_content.content\"\"\"\n",
    "        image_bytes = io.BytesIO(image_content)\n",
    "        file_path = os.path.join(download_path,filename)\n",
    "        image = Image.open(image_bytes)\n",
    "        _image=image.convert(\"RGB\")\n",
    "        with open(file_path,\"wb\") as f:\n",
    "            _image.save(f,\"JPEG\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception is \", e)\n",
    "        \n",
    "        \n",
    "def download_images(download_folder_path,urls,recipe,recipe_index,train_imgs_count):\n",
    "    try:\n",
    "        recipe_folder_name = str(recipe_index) + \"_\" + recipe\n",
    "        train_folder_path = os.path.join(download_folder_path,\"train\",recipe_folder_name)\n",
    "        test_folder_path = os.path.join(download_folder_path,\"test\",recipe_folder_name)\n",
    "\n",
    "        if not os.path.exists(train_folder_path):\n",
    "            os.makedirs(train_folder_path)\n",
    "        if not os.path.exists(test_folder_path):\n",
    "            os.makedirs(test_folder_path)\n",
    "            \n",
    "        img_number = 1\n",
    "        for url in urls:\n",
    "            if img_number <= train_imgs_count:\n",
    "                download_image(train_folder_path,url,str(img_number)+\"_\"+recipe+\".jpg\")\n",
    "            else:\n",
    "                download_image(test_folder_path,url,str(img_number)+\"_\"+recipe+\".jpg\")\n",
    "            img_number+=1\n",
    "    except Exception as e:\n",
    "        print(\"Exception is \", e)\n",
    "        #log_urls(image_urls,recipe_index_csv,recipe,\"dwd_logs.txt\",no_of_img_to_train)\n",
    "        \n",
    "def log_urls(image_urls,recipe_index_csv,recipe,path_to_download_logs,train_imgs_count,ignore_msgs=False):\n",
    "    try:\n",
    "        with open(path_to_download_logs,\"a\") as f:\n",
    "            f.write(str(recipe_index_csv)+\"->\"+ recipe +\"\\n\")\n",
    "            f.write(\"$$$$\\n\") # mark the start delimiter\n",
    "            logged_urls = 0\n",
    "            for url in image_urls:\n",
    "                if logged_urls < train_imgs_count:\n",
    "                    f.write(\"train:>\"+url+\"\\n\")\n",
    "                else:\n",
    "                    f.write(\"test:>\" + url+\"\\n\")\n",
    "                logged_urls+=1\n",
    "            f.write(\"$$$$\\n\")\n",
    "            f.write(\"\\n\") # one empty line between end delimiter and new recipe\n",
    "            f.flush()\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not ignore_msgs:\n",
    "            print(\"Exception occurred while logging urls\\n\",e)\n",
    "            \n",
    "                    \n",
    "# start web browser browser = webdriver.Chrome()\n",
    "# change to joinpath\n",
    "PATH=\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\venv\\\\chromedriver.exe\"\n",
    "chrome_service=webdriver.ChromeService(executable_path=PATH)\n",
    "chrome_options=Options()\n",
    "chrome_options.add_experimental_option(\"detach\",True)\n",
    "\n",
    "\n",
    "try:\n",
    "    # wait for search results to load\n",
    "    start = 334\n",
    "    row_count= 334\n",
    "    big_delay = 7\n",
    "    small_delay = 3\n",
    "    max_imgs = 10\n",
    "    extra_imgs = 5\n",
    "    batch_size = 10\n",
    "    no_of_img_to_train=8\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"linksc.csv\",skiprows= start,nrows=row_count,names=['name','link'])\n",
    "    d_log=open(\"dwd_logs.txt\",\"a\")\n",
    "    \n",
    "    # download images\n",
    "    for i in range(len(df['name'])):\n",
    "        recipe=df.loc[i,'name']\n",
    "        recipe_index_csv=i+start\n",
    "        \n",
    "        #d_log.write(str(recipe_index_csv)+\"->\"+ recipe +\"\\n\")\n",
    "        d_log.flush()\n",
    "        \n",
    "        driver=webdriver.Chrome(service=chrome_service,options=chrome_options)\n",
    "        driver.maximize_window()\n",
    "        driver.get('https://images.google.com/')\n",
    "        time.sleep(7)\n",
    "        search_box = driver.find_element(By.NAME,\"q\")\n",
    "        search_box.send_keys(recipe)\n",
    "        search_box.send_keys(Keys.ENTER)\n",
    "        time.sleep(10)\n",
    "    \n",
    "        image_urls=list()\n",
    "        # click on the first image\n",
    "        thumbnails = driver.find_elements(By.CLASS_NAME,\"mNsIhb\")\n",
    "        \"\"\"while len(thumbnails) < (max_imgs + extra_imgs): # checking condition before scrolling to minimize HTTP Requests\n",
    "            print(\"Less images obtained\")\n",
    "            time.sleep(small_delay)\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "            time.sleep(small_delay)\n",
    "            thumbnails = driver.find_elements(By.CLASS_NAME,\"mNsIhb\") \n",
    "        \"\"\"    \n",
    "        x=len(image_urls)\n",
    "        count=0\n",
    "        for thumbnail in thumbnails:\n",
    "            if(count==max_imgs):\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                thumbnail.click()\n",
    "                # wait for image to load\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                continue\n",
    "            #print(thumbnail)\n",
    "            pop_up_window = driver.find_elements(By.CLASS_NAME,\"jlTjKd\")  \n",
    "            for pop_up_elements in pop_up_window:\n",
    "                if pop_up_elements.tag_name == 'a':\n",
    "                    # print(\"<a> tag obtained\")\n",
    "\n",
    "                    # get all the img tags under <a>\n",
    "                    img_tags = pop_up_elements.find_elements(By.TAG_NAME,\"img\")\n",
    "\n",
    "                    # print(\"img tags obtained\")\n",
    "                    for img in img_tags:\n",
    "                        class_name=img.get_attribute(\"class\")\n",
    "                        if \"iPVvYb\" in class_name.strip():\n",
    "                            image_url=img.get_attribute('src')\n",
    "                            if(image_url not in image_urls):\n",
    "                                image_urls.append(image_url)\n",
    "                                print(\"Found \",count)\n",
    "                                count+=1\n",
    "                                \n",
    "                                # f.write(\"Found \" + str(success_count) + \"\\n\")\n",
    "                            else:\n",
    "                                print(\"Found Duplicate\")\n",
    "        \n",
    "        download_images(\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\DEEP-CHEF-PROJECT\\\\downloaded_images\",image_urls,recipe,recipe_index_csv,no_of_img_to_train)\n",
    "                    \n",
    "        log_urls(image_urls,recipe_index_csv,recipe,\"dwd_logs.txt\",no_of_img_to_train)        \n",
    "                # go back to search results\n",
    "        time.sleep(7)        \n",
    "    # close web browser\n",
    "    #print(image_urls)\n",
    "    d_log.close()\n",
    "except Exception as e:\n",
    "    print(\"Exception-\",e)\n",
    "    driver.quit()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
