{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flagged to be rev \\n1.)339 Delhi Potatoes\\n2.)340\\n3.)353\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"flagged to be rev \n",
    "1.)339 Delhi Potatoes\n",
    "2.)340\n",
    "3.)353\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified till 445\n",
    "#flagged 422 just pudla\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating driver for scraping and downloading recipes' images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "import urllib3\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  0\n",
      "Found  1\n",
      "Found Duplicate\n",
      "Found  2\n",
      "Found  3\n",
      "Found  4\n",
      "Found  5\n",
      "Found  6\n",
      "not able to load img \n",
      "Found  7\n",
      "Found  8\n",
      "Found  9\n",
      "Found  0\n",
      "Found  1\n",
      "Found Duplicate\n",
      "Found  2\n",
      "Found  3\n",
      "Found  4\n",
      "Found  5\n",
      "Found  6\n",
      "Found  7\n",
      "Found Duplicate\n",
      "Found Duplicate\n",
      "Found  8\n",
      "Found  9\n",
      "Found  0\n",
      "Found Duplicate\n",
      "not able to load img \n",
      "Found  1\n",
      "Found  2\n",
      "Found  3\n",
      "Found  4\n",
      "Found  5\n",
      "Found  6\n",
      "Found  7\n",
      "Found  8\n",
      "Found  9\n",
      "Found  0\n",
      "Found  1\n",
      "Found  2\n",
      "not able to load img \n",
      "Found  3\n",
      "Found  4\n",
      "Found  5\n",
      "Found  6\n",
      "Found  7\n",
      "not able to load img \n",
      "Found  8\n",
      "Found  9\n",
      "Found  0\n",
      "Found  1\n",
      "Found  2\n",
      "Found  3\n",
      "Found  4\n",
      "not able to load img \n",
      "Found  5\n",
      "Found  6\n",
      "Found Duplicate\n",
      "Found  7\n",
      "Found Duplicate\n",
      "Found  8\n",
      "not able to load img \n",
      "Found  9\n"
     ]
    }
   ],
   "source": [
    "#335 to 668 incl\n",
    "\n",
    "#checking if the image can load with a good value of timeout\n",
    "def fetch_image_content_from_url(url,timeout):\n",
    "    try:\n",
    "        http = urllib3.PoolManager(timeout=urllib3.Timeout(connect=None, read=None, total=timeout))\n",
    "        response = http.request('GET', url)\n",
    "        return response.data\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred inside fetch_image_content_from_url\\n\",e)\n",
    "        \n",
    "\n",
    "#download particular image and saving as 'filename'   \n",
    "def download_image(download_path,url,filename):\n",
    "    try:\n",
    "        image_content = fetch_image_content_from_url(url,200)#fetching content of the image with a 200s timeout loading time\n",
    "        #requests.get(url,timeout=10)\n",
    "        image_bytes = io.BytesIO(image_content)#converting to bytes\n",
    "        file_path = os.path.join(download_path,filename)#creating filepath for image\n",
    "        image = Image.open(image_bytes)#opening the image\n",
    "        _image=image.convert(\"RGB\")#converting to RGB format since png format(which contains RGBA) gives error while downloading\n",
    "        with open(file_path,\"wb\") as f:\n",
    "            _image.save(f,\"JPEG\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception is \", e)\n",
    "        \n",
    "        \n",
    "def download_images(download_folder_path,urls,recipe,recipe_index,train_imgs_count):\n",
    "    try:\n",
    "        recipe_folder_name = str(recipe_index) + \"_\" + recipe\n",
    "        train_folder_path = os.path.join(download_folder_path,\"train\",recipe_folder_name)\n",
    "        test_folder_path = os.path.join(download_folder_path,\"test\",recipe_folder_name)\n",
    "\n",
    "        if not os.path.exists(train_folder_path):\n",
    "            os.makedirs(train_folder_path)\n",
    "        if not os.path.exists(test_folder_path):\n",
    "            os.makedirs(test_folder_path)\n",
    "            \n",
    "        img_number = 1\n",
    "        for url in urls:\n",
    "            if img_number <= train_imgs_count:\n",
    "                download_image(train_folder_path,url,str(img_number)+\"_\"+recipe+\".jpg\")\n",
    "            else:\n",
    "                download_image(test_folder_path,url,str(img_number)+\"_\"+recipe+\".jpg\")\n",
    "            img_number+=1\n",
    "    except Exception as e:\n",
    "        print(\"Exception is \", e)\n",
    "        #log_urls(image_urls,recipe_index_csv,recipe,\"dwd_logs.txt\",no_of_img_to_train)\n",
    "        \n",
    "def log_urls(image_urls,recipe_index_csv,recipe,path_to_download_logs,train_imgs_count,ignore_msgs=False):\n",
    "    try:\n",
    "        with open(path_to_download_logs,\"a\") as f:\n",
    "            f.write(str(recipe_index_csv)+\"->\"+ recipe +\"\\n\")\n",
    "            f.write(\"$$$$\\n\") # mark the start delimiter\n",
    "            logged_urls = 0\n",
    "            for url in image_urls:\n",
    "                if logged_urls < train_imgs_count:\n",
    "                    f.write(\"train:>\"+url+\"\\n\")\n",
    "                else:\n",
    "                    f.write(\"test:>\" + url+\"\\n\")\n",
    "                logged_urls+=1\n",
    "            f.write(\"$$$$\\n\")\n",
    "            f.write(\"\\n\") # one empty line between end delimiter and new recipe\n",
    "            f.flush()\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not ignore_msgs:\n",
    "            print(\"Exception occurred while logging urls\\n\",e)\n",
    "            \n",
    "driver=None   \n",
    "        \n",
    "def create_driver_and_load(cService,Options):\n",
    "    global driver\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=cService,options=Options)\n",
    "        driver.maximize_window()\n",
    "        driver.get('https://images.google.com/')\n",
    "        time.sleep(7)\n",
    "    except Exception as e :\n",
    "        print(\"Exception occurred creating driver \\n\",e)         \n",
    "\n",
    "# change to joinpath\n",
    "PATH=\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\venv\\\\chromedriver.exe\"\n",
    "chrome_service=webdriver.ChromeService(executable_path=PATH)\n",
    "chrome_options=Options()\n",
    "chrome_options.add_experimental_option(\"detach\",True)\n",
    "\n",
    "\n",
    "try:\n",
    "    # wait for search results to load\n",
    "    start = 440\n",
    "    row_count= 5\n",
    "    big_delay = 7\n",
    "    small_delay = 3\n",
    "    max_imgs = 10 \n",
    "    extra_imgs = 5\n",
    "    #batch_size = 2#change to 10 later\n",
    "    no_of_img_to_train=8\n",
    "    max_attempts_driver=0\n",
    "\n",
    "    create_driver_and_load(chrome_service,chrome_options)\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\DEEP-CHEF-PROJECT\\\\links_copy_main.csv\",skiprows= start,nrows=row_count,names=['name','link'])\n",
    "    d_log=open(\"dwd_logs.txt\",\"a\")\n",
    "    \n",
    "    # download images\n",
    "    for i in range(len(df['name'])):\n",
    "        recipe=df.loc[i,'name']\n",
    "        recipe_name=recipe\n",
    "        recipe=recipe_name + \" Recipe OR \" + recipe_name + \" Dish\"\n",
    "        recipe_index_csv=i+start\n",
    "        \n",
    "        #d_log.write(str(recipe_index_csv)+\"->\"+ recipe +\"\\n\")\n",
    "        d_log.flush()\n",
    "        \n",
    "        search_box = driver.find_element(By.NAME,\"q\")\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(recipe)\n",
    "        search_box.send_keys(Keys.ENTER)\n",
    "        time.sleep(10)\n",
    "    \n",
    "        image_urls=list()\n",
    "        # click on the first image\n",
    "        thumbnails = driver.find_elements(By.CLASS_NAME,\"mNsIhb\")\n",
    "        \n",
    "        \n",
    "        if(len(thumbnails)<(max_imgs + extra_imgs)):\n",
    "            print(\"quitting driver\")\n",
    "            driver.quit()\n",
    "            print(\"creating new driver\")\n",
    "            create_driver_and_load(chrome_service,chrome_options)\n",
    "            max_attempts_driver+=1\n",
    "            if(max_attempts_driver>=3):\n",
    "                raise\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        \n",
    "            \n",
    "        x=len(image_urls)\n",
    "        count=0\n",
    "        for thumbnail in thumbnails:\n",
    "            if(count==max_imgs):\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                action = ActionChains(driver)\n",
    "                action.move_to_element(thumbnail).click().perform()\n",
    "                # wait for image to load\n",
    "                time.sleep(10)\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"Stale element encountered while clicking thumbnail. Refreshing page...\")\n",
    "                \"\"\"driver.refresh()\n",
    "                thumbnails = driver.find_elements(By.CLASS_NAME,\"mNsIhb\")  # Refetch thumbnails\n",
    "                thumbnail=thumbnails[i]\"\"\"\n",
    "                #wait.until(ExpectedConditions.presenceOfElementLocated(By.CLASS_NAME(\"mNsIhb\")));\n",
    "                time.sleep(5)\n",
    "                pass\n",
    "            except: \n",
    "                continue  \n",
    "            #print(thumbnail)\n",
    "            pop_up_window = driver.find_elements(By.CLASS_NAME,\"jlTjKd\")  \n",
    "            for pop_up_elements in pop_up_window:\n",
    "                if pop_up_elements.tag_name == 'a':\n",
    "                    # print(\"<a> tag obtained\")\n",
    "\n",
    "                    # get all the img tags under <a>\n",
    "                    img_tags = pop_up_elements.find_elements(By.TAG_NAME,\"img\")\n",
    "\n",
    "                    # print(\"img tags obtained\")\n",
    "                    for img in img_tags:\n",
    "                        class_name=img.get_attribute(\"class\")\n",
    "                        if \"iPVvYb\" in class_name.strip():\n",
    "                            image_url=img.get_attribute('src')\n",
    "                            if(image_url not in image_urls):\n",
    "                                if(requests.get(image_url).status_code==200):        \n",
    "                                    image_urls.append(image_url)\n",
    "                                    print(\"Found \",count)\n",
    "                                    count+=1\n",
    "                                else:\n",
    "                                    print(\"not able to load img \")    \n",
    "                                    continue\n",
    "                                \n",
    "                                # f.write(\"Found \" + str(success_count) + \"\\n\")\n",
    "                            else:\n",
    "                                print(\"Found Duplicate\")\n",
    "        \n",
    "        download_images(\"C:\\\\Users\\\\Aarush Raj\\\\OneDrive\\\\Desktop\\\\img2rec\\\\DEEP-CHEF-PROJECT\\\\downloaded_images\",image_urls,recipe_name,recipe_index_csv,no_of_img_to_train)\n",
    "                    \n",
    "        log_urls(image_urls,recipe_index_csv,recipe_name,\"dwd_logs.txt\",no_of_img_to_train)        \n",
    "                # go back to search results\n",
    "        time.sleep(7)        \n",
    "    # close web browser\n",
    "    #print(image_urls)\n",
    "    d_log.close()\n",
    "except Exception as e:\n",
    "    print(\"Exception-\",e)\n",
    "    driver.quit()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
